{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b222186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"skills_dataset.csv\")\n",
    "\n",
    "def collect_columns(prefix):\n",
    "    cols = [c for c in df.columns if c.startswith(prefix)]\n",
    "    return df[cols].fillna(\"\").agg(\", \".join, axis=1)\n",
    "\n",
    "df[\"prerequisites_text\"] = collect_columns(\"prerequisites/\")\n",
    "df[\"complementary_text\"] = collect_columns(\"complementary_skills/\")\n",
    "df[\"industry_text\"] = collect_columns(\"industry_usage/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77910cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Optional fine-tuning configuration. Set `do_finetune = True` to run epochs.\n",
    "do_finetune = False\n",
    "finetune_epochs = 30\n",
    "finetune_batch_size = 16\n",
    "save_finetuned_model = False\n",
    "finetuned_model_path = \"fine_tuned_sbert\"\n",
    "\n",
    "if do_finetune:\n",
    "    from sentence_transformers import InputExample, losses\n",
    "    from torch.utils.data import DataLoader\n",
    "    # Prepare positive pairs: skill text <-> complementary/prerequisite context\n",
    "    examples = []\n",
    "    for _, r in df.iterrows():\n",
    "        skill_text = f\"Skill: {r['skill_name']} Category: {r['category']} Industry: {r['industry_text']}\"\n",
    "        pos = r['complementary_text'] if pd.notna(r['complementary_text']) and str(r['complementary_text']).strip() else r['prerequisites_text']\n",
    "        if pd.isna(pos) or not str(pos).strip():\n",
    "            continue\n",
    "        pos_text = f\"Context: {pos}\"\n",
    "        examples.append(InputExample(texts=[skill_text, pos_text]))\n",
    "\n",
    "    if len(examples) == 0:\n",
    "        print(\"No training examples found; skipping fine-tune\")\n",
    "    else:\n",
    "        train_dataloader = DataLoader(examples, shuffle=True, batch_size=finetune_batch_size)\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "        warmup_steps = max(100, int(len(train_dataloader) * finetune_epochs * 0.1))\n",
    "        model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=finetune_epochs, warmup_steps=warmup_steps)\n",
    "        if save_finetuned_model:\n",
    "            model.save(finetuned_model_path)\n",
    "\n",
    "def build_weighted_embedding(row):\n",
    "    skill_emb = model.encode(\n",
    "        f\"Skill: {row['skill_name']}\",\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    context_emb = model.encode(\n",
    "        f\"\"\"\n",
    "        Category: {row['category']}\n",
    "        Industry Usage: {row['industry_text']}\n",
    "        Complementary Skills: {row['complementary_text']}\n",
    "        \"\"\",\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    market_emb = model.encode(\n",
    "        f\"\"\"\n",
    "        Job Demand Score: {row['job_demand_score']}\n",
    "        Future Relevance Score: {row['future_relevance_score']}\n",
    "        Market Trend: {row['market_trend']}\n",
    "        \"\"\",\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        0.5 * skill_emb +\n",
    "        0.3 * context_emb +\n",
    "        0.2 * market_emb\n",
    "    )\n",
    "\n",
    "df[\"embedding\"] = df.apply(build_weighted_embedding, axis=1)\n",
    "embeddings = np.vstack(df[\"embedding\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "770d46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "152b649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_by_prerequisites(df, user_skills):\n",
    "    # Keep this helper if you want to strictly filter out skills that require\n",
    "    # prerequisites the user doesn't have. For recommendations based on\n",
    "    # matching user skills we'd rather compute overlap than hard-filter.\n",
    "    def prereq_ok(prereqs):\n",
    "        if not prereqs:\n",
    "            return True\n",
    "        prereq_list = [p.strip().lower() for p in prereqs.split(\",\")]\n",
    "        return any(skill.lower() in prereq_list for skill in user_skills)\n",
    "\n",
    "    # Return boolean mask if strict filtering is needed elsewhere\n",
    "    return df[df[\"prerequisites_text\"].apply(prereq_ok)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4af5b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User profile embeddings removed.\n",
    "# Recommendations build a query embedding by averaging existing skill embeddings\n",
    "# for the user's known skills (avoids encoding arbitrary user text).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1408a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_score(row, similarity, match_score=0.0):\n",
    "    # Combine semantic similarity, market signals and explicit skill matches.\n",
    "    # We give extra weight to match_score so user skills influence ranking.\n",
    "    sim_w = 0.45\n",
    "    demand_w = 0.15\n",
    "    future_w = 0.15\n",
    "    match_w = 0.25\n",
    "    return (\n",
    "        sim_w * similarity +\n",
    "        demand_w * (row[\"job_demand_score\"] / 100) +\n",
    "        future_w * (row[\"future_relevance_score\"] / 100) +\n",
    "        match_w * match_score\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5705c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_skills(\n",
    "    user_skills,\n",
    "    top_k=10\n",
    "):\n",
    "\n",
    "    # Build a query embedding by averaging embeddings of the user's known skills\n",
    "    user_set = set([s.strip().lower() for s in user_skills if s.strip()])\n",
    "    mask = df['skill_name'].fillna('').str.lower().isin(user_set)\n",
    "    if mask.any():\n",
    "        user_query_emb = np.vstack(df.loc[mask, 'embedding'].values).mean(axis=0)\n",
    "    else:\n",
    "        # fallback: use mean of all embeddings (less specific)\n",
    "        user_query_emb = np.mean(embeddings, axis=0)\n",
    "\n",
    "    # retrieve a larger candidate pool from FAISS and re-rank using structured signals\n",
    "    scores, indices = index.search(\n",
    "        user_query_emb.reshape(1, -1),\n",
    "        k=200  # retrieve more, re-rank and dedupe below\n",
    "    )\n",
    "\n",
    "    candidates = df.iloc[indices[0]].copy().reset_index(drop=True)\n",
    "    candidates[\"similarity\"] = scores[0]\n",
    "\n",
    "    # compute overlaps between user skills and candidate text fields\n",
    "    user_set = set([s.strip().lower() for s in user_skills if s.strip()])\n",
    "\n",
    "    def overlap_count(text):\n",
    "        if not text or pd.isna(text):\n",
    "            return 0\n",
    "        toks = set([t.strip().lower() for t in text.split(\",\") if t.strip()])\n",
    "        return len(user_set & toks)\n",
    "\n",
    "    candidates[\"prereq_overlap\"] = candidates[\"prerequisites_text\"].fillna(\"\").apply(overlap_count)\n",
    "    candidates[\"comp_overlap\"] = candidates[\"complementary_text\"].fillna(\"\").apply(overlap_count)\n",
    "    candidates[\"skill_name_match\"] = candidates[\"skill_name\"].fillna(\"\").apply(lambda s: 1 if s.strip().lower() in user_set else 0)\n",
    "\n",
    "    denom = max(1, len(user_set))\n",
    "    # weighted match score: favour exact skill-name matches, then prereqs, then complementary skills\n",
    "    candidates[\"match_score\"] = (\n",
    "        0.6 * candidates[\"skill_name_match\"] +\n",
    "        0.3 * (candidates[\"prereq_overlap\"] / denom) +\n",
    "        0.1 * (candidates[\"comp_overlap\"] / denom)\n",
    "    )\n",
    "\n",
    "    # apply structured scoring that includes the match_score\n",
    "    candidates[\"final_score\"] = candidates.apply(\n",
    "        lambda r: structured_score(r, r[\"similarity\"], r[\"match_score\"]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # dedupe by skill name and return top-k\n",
    "    result = candidates.sort_values(\"final_score\", ascending=False).drop_duplicates(subset=[\"skill_name\"]).head(top_k).reset_index(drop=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c27810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'Python Programming', 'Django', 'Python Programming (for ML)', 'Machine Learning Frameworks (TensorFlow, PyTorch)', 'Artificial Intelligence (AI) & Machine Learning (ML)', 'Flask', 'Deep Learning Frameworks (TensorFlow, PyTorch)', 'Artificial Intelligence (AI) Programming', 'Data Structures & Algorithms (DSA)', 'Scikit-learn', 'Data Structures & Algorithms', 'Scripting Languages (Shell, Python)', 'Python for Finance', 'Scripting Languages (Python, Go, Ruby)', 'Regression Analysis', 'R (Statistical Programming Language)', 'Algorithms', 'Data Structures', 'TensorFlow']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_skills = [\"Python\", \"DSA\"]\n",
    "\n",
    "recommendations = recommend_skills( user_skills, top_k=20)\n",
    "print(recommendations['skill_name'].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
